{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seunguk_(3)_lyricModelling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1jCp4ul4t1aM0jmbl4X6kauP7yt8HBIu8",
      "authorship_tag": "ABX9TyNQtjNzHpgMW3yIkJOzaxJj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sihyeon3523/Six_of_cells/blob/modeling/Seunguk_(3)_lyricModelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# 0. Install & import libraries "
      ],
      "metadata": {
        "id": "gX_jZ07Dynfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output \n",
        "\n",
        "!pip install transformers==4.8.2\n",
        "!pip install sentencepiece==0.1.96\n",
        "!pip install tensorflow_addons\n",
        "\n",
        "clear_output()\n",
        "\n",
        "import sklearn\n",
        "import tensorflow\n",
        "import transformers\n",
        "import tensorflow_addonss\n",
        "\n",
        "print(sklearn.__version__) # 1.0.2\n",
        "print(tensorflow.__version__) # 2.8.0\n",
        "print(transformers.__version__) # 4.8.2\n",
        "print(tensorflow_addons.__version__) # 0.16.1\n",
        "\n",
        "import sentencepiece\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import re\n",
        "import pickle \n",
        "import dill # for saving a function as a file(.pkl)\n",
        "import logging # for changing the tf's logging level\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn import model_selection\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa # for using Rectified-Adam optimizer (instead of Adam optimizer) \n",
        "from tensorflow.keras import layers, initializers, losses, optimizers, metrics, callbacks \n",
        "\n",
        "import transformers\n",
        "from transformers import TFBertModel # BertTokenizer 제외\n",
        "\n",
        "import sentencepiece as spm\n",
        "import ast  # str list to list"
      ],
      "metadata": {
        "id": "WYMLcxGyy6R4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "5e87aee2-46bd-4844-c90b-c390063991d3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a53888d5bcf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_addonss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 1.0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_addonss'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random seed 고정\n",
        "tf.random.set_seed(1234)\n",
        "np.random.seed(1234)\n",
        "\n",
        "# Transformers logging level 변경 (WARNING -> ERROR) @ https://huggingface.co/transformers/main_classes/logging.html\n",
        "transformers.logging.set_verbosity(transformers.logging.ERROR)\n",
        "\n",
        "# Tensorflow logging level 변경 \n",
        "tf.get_logger().setLevel(logging.ERROR)"
      ],
      "metadata": {
        "id": "ZP1nP5NXBT8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KoBERT 불러오기\n",
        "!git clone https://github.com/monologg/KoBERT-Transformers.git\n",
        "!mv KoBERT-Transformers/kobert_transformers/tokenization_kobert.py /content\n",
        "clear_output() # clear the output\n",
        "\n",
        "#토크나이저 생성\n",
        "from tokenization_kobert import KoBertTokenizer \n",
        "\n",
        "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')"
      ],
      "metadata": {
        "id": "du0ZRI16BagI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Google Drive "
      ],
      "metadata": {
        "id": "X6qL5EKSBowl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pEPEnIFu94xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "metadata": {
        "id": "DFlih56_7FbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 경로만 바꾸면 됨!\n",
        "path = '/content/drive/MyDrive/likelion/proj_fin_seunguk'# google drive project path\n",
        "data_path = 'colab_data/temp_data/' # model data path\n",
        "checkpoint_path = 'colab_data/temp_data/saved_models/' # saved model path\n",
        "\n",
        "os.chdir(path)"
      ],
      "metadata": {
        "id": "EaiTyc8n5Oev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load the Data"
      ],
      "metadata": {
        "id": "35DqagFHBmlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_allsongs_original = pd.read_csv('df_allsongs.csv', lineterminator='\\n')"
      ],
      "metadata": {
        "id": "tA1axB-LzH_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_allsongs_original.head(3)"
      ],
      "metadata": {
        "id": "dYC3vZoJzMyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_allsongs = df_allsongs_original[[\"Title\", \"Artist\", \"Date\", \"Genre\", \"preprocess_Lyric_ver3\"]]\n",
        "df_allsongs"
      ],
      "metadata": {
        "id": "vzWVzsGt0-hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_allsongs.info())\n",
        "print(df_allsongs.describe())"
      ],
      "metadata": {
        "id": "VcmkX5k9Hjgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_allsongs.preprocess_Lyric_ver3.iloc[12]"
      ],
      "metadata": {
        "id": "CIv6nQUhhFwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load saved Model\n"
      ],
      "metadata": {
        "id": "5AH_bjFE_DlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Load the Model-builder (function)\n",
        "with open(data_path + 'model_BERTfunction_v1.pkl', 'rb') as f:\n",
        "    create_model = dill.loads(pickle.load(f)) # use dill to pickle a python function\n",
        "\n",
        "# 2) Load the Bert-tokenizer \n",
        "with open(data_path + 'tokenizer-bert.pkl', 'rb') as f:\n",
        "    tokenizer = pickle.load(f) \n",
        "\n",
        "\n",
        "# 3) Create the model & load the Model-weights (from checkpoint file)\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.TPUStrategy(resolver) # Obsolete : tf.distribute.experimental.TPUStrategy()\n",
        "\n",
        "with strategy.scope(): \n",
        "    model = create_model(max_length=300) \n",
        "\n",
        "model.load_weights(filepath=checkpoint_path + 'best_bert_weights.h5')"
      ],
      "metadata": {
        "id": "IqkTwemez5n3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifier as a function"
      ],
      "metadata": {
        "id": "yYz-CbvMAJ1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment_element(sentence, tokenizer, model):\n",
        "    \"\"\"\n",
        "    가사 내 문장 별 확률 값 추출\n",
        "    \"\"\"    \n",
        "    SEQ_LEN = 300 \n",
        "\n",
        "    # Tokenizing / Tokens to sequence numbers / Padding\n",
        "    encoded_dict = tokenizer.encode_plus(text=re.sub(\"[^\\s0-9a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣]\", \"\", sentence), # 특수문자 제거\n",
        "                                         padding='max_length', \n",
        "                                         truncation = True,\n",
        "                                         max_length=SEQ_LEN) \n",
        "    \n",
        "    token_ids = np.array(encoded_dict['input_ids']).reshape(1, -1) \n",
        "    token_masks = np.array(encoded_dict['attention_mask']).reshape(1, -1)\n",
        "    token_segments = np.array(encoded_dict['token_type_ids']).reshape(1, -1)\n",
        "    \n",
        "    new_inputs = (token_ids, token_masks, token_segments)\n",
        "\n",
        "    # Prediction\n",
        "    pred = model.predict(new_inputs) \n",
        "    all_pred_proba = np.round(pred * 100, 2)\n",
        "    \n",
        "    #top_pred_proba = np.round(np.max(pred) * 100, 2) \n",
        "    #pred_class = ['분노혐오','놀람공포','슬픔','행복'][np.argmax(pred, axis=1)[0]] # 수정하면 4개 클래스 다 나올듯! \n",
        "    #print(\"{}% 확률로 {} 텍스트입니다.\".format(top_pred_proba, pred_class))\n",
        "\n",
        "    return all_pred_proba "
      ],
      "metadata": {
        "id": "rldmfWAZAHK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def total_sentiment(lyrics):\n"
      ],
      "metadata": {
        "id": "cL2_C50bU7_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TEST temporary"
      ],
      "metadata": {
        "id": "gfhNeSNP2hYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprocess_Lyric_ver3에 적용 <br> \n",
        "\n",
        "list가 str 형태로 row에 저장되었기 때문에 ast library로 다시 list로 바꿔주어야한다.<br>\n",
        "(str list to list)"
      ],
      "metadata": {
        "id": "rWzyg_PK-Mbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp_list = df_allsongs.preprocess_Lyric_ver3.iloc[0]\n",
        "print(type(temp_list))\n",
        "temp_list"
      ],
      "metadata": {
        "id": "CrHuHg912f6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast  \n",
        "\n",
        "temp_list_2 = ast.literal_eval(temp_list) \n",
        "print(type(temp_list_2)) \n",
        "print(temp_list_2)\n",
        "\n",
        "## str list to list\n",
        "## sentence = ast.literal_eval(sentence)"
      ],
      "metadata": {
        "id": "owQngh5w9dYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(temp_list_2))\n",
        "print(temp_list_2[1])\n",
        "all_pred_proba = predict_sentiment_element(temp_list_2[1], tokenizer, model)\n",
        "print()\n",
        "print(all_pred_proba)"
      ],
      "metadata": {
        "id": "uo8rAk4u-Bz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anger = all_pred_proba[0][0]\n",
        "scary = all_pred_proba[0][1]\n",
        "sad = all_pred_proba[0][2]\n",
        "happy = all_pred_proba[0][3]\n",
        "\n",
        "total = np.ndarray.round( (np.array([(anger, scary, sad, happy)]) / (anger + scary + sad + happy)), 2)\n",
        "\n",
        "top_pred_prob = total[0][np.argmax(total, axis=1)[0]]"
      ],
      "metadata": {
        "id": "Bqj7_ThWMwKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(total)\n",
        "print(top_pred_prob)  "
      ],
      "metadata": {
        "id": "GPyd2l0czmEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_list_2 = df_allsongs.preprocess_Lyric_ver3.iloc[12]\n",
        "temp_list_2 = ast.literal_eval(temp_list_2) \n",
        "print(len(temp_list_2))\n",
        "print(temp_list_2[-1])\n",
        "print(temp_list_2)"
      ],
      "metadata": {
        "id": "3SbVMCv1_vg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 마지막 원소 '' 빈칸임. <br/> 태그 기준으로 split해서 나옴, 빈칸인 element 제거\n",
        "# 가사내 문장 단위로 predict_sentiment_element 함수 적용 \n",
        "# 앞으로 temp_list_2 자리에 row 넣으면 됨\n",
        "\n",
        "    # print(len(temp_list_2)) \n",
        "    # for i in range(len(temp_list_2)):\n",
        "        \n",
        "    #     # 빈칸 제거\n",
        "    #     if temp_list_2[i] == '':\n",
        "    #         del temp_list_2[i]  \n",
        "\n",
        "    #     elif temp_list_2[i] != '':\n",
        "    #         all_pred_proba = predict_sentiment_element(temp_list_2[i], tokenizer, model)\n",
        "    #         anger = all_pred_proba[0][0]\n",
        "    #         scary = all_pred_proba[0][1]\n",
        "    #         sad = all_pred_proba[0][2]\n",
        "    #         happy = all_pred_proba[0][3]\n",
        "    #         #print(i, temp_list_2[i], all_pred_proba) \n",
        "\n",
        "    #     anger += anger \n",
        "    #     scary += scary\n",
        "    #     sad  += sad\n",
        "    #     happy += happy\n",
        "\n",
        "    # total = np.ndarray.round( (np.array([(anger, scary, sad, happy)]) / (anger + scary + sad + happy)), 2)\n",
        "    # top_pred_prob = total[0][np.argmax(total, axis=1)[0]]\n",
        "    # top_pred_class = ['분노혐오','놀람공포','슬픔','행복'][np.argmax(total, axis=1)[0]]\n",
        "\n",
        "    # print(total)\n",
        "    # print(top_pred_class) \n",
        "    # print(top_pred_prob)\n",
        "    # print(len(temp_list_2))"
      ],
      "metadata": {
        "id": "VQqjq3ZZShwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 마지막 원소 '' 빈칸임. <br/> 태그 기준으로 split해서 나옴, 빈칸인 element 제거\n",
        "# 가사내 문장 단위로 predict_sentiment_element 함수 적용 \n",
        "# 앞으로 temp_list_2 자리에 row 넣으면 됨\n",
        "\n",
        "def predict_sentiment_lyrics(lyrics):\n",
        "    print(len(lyrics)) \n",
        "    for i in range(len(lyrics)):\n",
        "        \n",
        "        # 빈칸 제거\n",
        "        if lyrics[i] == '':\n",
        "            del lyrics[i]  \n",
        "\n",
        "        elif lyrics[i] != '':\n",
        "            all_pred_proba = predict_sentiment_element(lyrics[i], tokenizer, model)\n",
        "            anger = all_pred_proba[0][0]\n",
        "            scary = all_pred_proba[0][1]\n",
        "            sad = all_pred_proba[0][2]\n",
        "            happy = all_pred_proba[0][3]\n",
        "            #print(i, lyrics[i], all_pred_proba) \n",
        "\n",
        "        anger += anger \n",
        "        scary += scary\n",
        "        sad  += sad\n",
        "        happy += happy\n",
        "\n",
        "    total_array = np.ndarray.round( (np.array([(anger, scary, sad, happy)]) / (anger + scary + sad + happy)), 2)\n",
        "    top_pred_prob = total_array[0][np.argmax(total_array, axis=1)[0]]\n",
        "    top_pred_class = ['분노혐오','놀람공포','슬픔','행복'][np.argmax(total_array, axis=1)[0]]\n",
        "\n",
        "    return total_array, top_pred_class, top_pred_prob"
      ],
      "metadata": {
        "id": "E4yssOZ02zTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply model"
      ],
      "metadata": {
        "id": "-MrmcLPG2od2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Emotion = []\n",
        "# Probability = []\n",
        "# 분노혐오 = []\n",
        "# 놀람공포 = []\n",
        "# 슬픔 = []\n",
        "# 행복 = []\n",
        "\n",
        "\n",
        "# for row in tqdm(df_allsongs.iterrows()):\n",
        "\n",
        "#         pred_class, top_pred_proba, all_pred_proba = predict_sentiment_element(row[1][-1], tokenizer, model)\n",
        "#         #break\n",
        "#         Emotion.append(pred_class)\n",
        "#         Probability.append(top_pred_proba)\n",
        "#         분노혐오.append(all_pred_proba[0][0])\n",
        "#         놀람공포.append(all_pred_proba[0][1])\n",
        "#         슬픔.append(all_pred_proba[0][2])\n",
        "#         행복.append(all_pred_proba[0][3])"
      ],
      "metadata": {
        "id": "jYYaHQ1bI0v6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_allsongs['Emotion'] = Emotion\n",
        "# df_allsongs['Proba'] = Probability\n",
        "# df_allsongs['분노혐오'] = 분노혐오\n",
        "# df_allsongs['놀람공포'] = 놀람공포\n",
        "# df_allsongs['슬픔'] = 슬픔\n",
        "# df_allsongs['행복'] = 행복"
      ],
      "metadata": {
        "id": "qMBNrBv-KO9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_allsongs['Emotion'].value_counts()"
      ],
      "metadata": {
        "id": "0tHP3YDqKgpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_allsongs"
      ],
      "metadata": {
        "id": "79dMUIMVhNro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_allsongs.to_csv('songtagged_1.csv')"
      ],
      "metadata": {
        "id": "WUbaWH4IUtqx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}